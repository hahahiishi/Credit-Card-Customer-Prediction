# -*- coding: utf-8 -*-
"""PROJECT 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WwTRZSB_rx2srAxjEAsxXad_tPDLr-xh

# CRISP-DM

1. BUSINESS UNDER STANDING
  * Permasalahan : terdapat lebih dari 20% nasabah kartu kredit finanku gagal bayar. Akibatnya oprasional bisnis menjadi terganggu
  * Tujuan bisnis: Finanku ingin mengetahui lebih awal nasabah lending yang berpotensi untuk gagal bayar
  * Tujuan analisa: membuat sebuah model prediksi gagal bayar untuk fasilitas kartu kredit finanku.
2. Data Understanding
  * Branch : lokasi cabang nasabah terdaftar
  * City : lokasi kota nasabah terdaftar
  * Age : usia
  * Avg Annual Income: rata - rata pendapatan tahunan nasabah
  * Balance rata rata nominal tabungan yang tersimpan
  * NumofProduct : jumlah kepemilikan produk nasabah finanku
  * HasCrCard : Surat kepemilikan kartu kredit
  * Active member: status keaktifan
3. Data Preparation
  * Pengecekan data duplikan // null
  * Transformasi data
  * Pengecekan korelasi
  * Menambah variabel / fitur
  * Mengulang tahapan yang sama untuk data set validasi
  * Pemisahan data train dan test
4. Modelling
  * Pemilihan Algoritma
  * Pencarian hyperparameter Terbaik
  * Membangun Model
5. Evaluation
  * Confusion Matrix
6. Deployment

## PROBLEM STATEMENT

Ke khawatiran adanya keterambatan pembayaran kartu kredit pada finanku yang merugikan bisnis. segingga orang orang memiliki potensi untuk terlambat membayar  bisa diprediksi cepat untuk menentukan strategi yang sesuai untuk menghadapi kondisi yang akan datang

## OBJECTIVE
Membuat model yang setidaknya dapat memprediksi 60% dari pelanggan yang telat bayar kartu kredit (accuracy dan recall diatas 60%)

## Variable yang tersedia
dari dataset yang dimiliki terdapat beberapa informasi yang tersedia:
1. Custumer ID
2. Branch
3. City
4. Age
5. avg annual income
6. Balance (Q1 - Q4)
7. Num of Product (Q1 - Q4)
8. Has CC (Q1 - Q4)
9. Active member ( Q1- Q4)
10. Unpaid Tagging

## EXPERIMENT
Periode Tinjauan
1. Nasabah di review selama satu tahun terakhir
2. Nasabah di review selama 6 bulan terakhir

Penyesuaian Variabel
1. Balance dilihan dari rata rata selama horizon waktu dan dilihat perubahan pada akhir tinjauan dan awal tinjauan
2. melihan kepemilikan dari jumlah produk rata rata, maksimum, minimum pada periode tinjauan
3. Status keaktifan nasabah dilihat dalam bentuk bulan

# PACKAGE
"""

# DATA
from google.colab import drive

#JCOP
!pip install jcopml
#BASIC
import pandas as pd
import numpy as np

#VISUALIZATION
import matplotlib.pyplot as plt
import seaborn as sns
import time
#MACHINE LEARNING
## MODEL
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.model_selection import GridSearchCV
from xgboost import XGBClassifier
from sklearn.metrics import confusion_matrix,classification_report, make_scorer, accuracy_score, precision_score, recall_score, f1_score
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
from jcopml.feature_importance import mean_score_decrease

#Warning
import warnings
warnings.filterwarnings('ignore')

drive.mount('/content/drive')
path = "drive/My Drive/Colab Notebooks/code"
dftrain = pd.read_csv(path + "/data/somed/test (1).csv")
test = pd.read_csv(path + "/data/somed/train (1).csv")
val = pd.read_csv(path + "/data/somed/val.csv")

# DATA SET EKSPERIMEN 1
df1 = df_all.copy()
df1.head()

#DATA SET EKSPERIMEN 2
df2 = df_all.copy()
df2.head()

"""# DATA UNDERSTANDING
Sebaran Nasabah Berdasarkan Lokasi
 * Sebaran Keseluruhan
 * Sebaran Nasabah Gagal Bayar

"""

data1 = pd.DataFrame(\
                      #Megelompokkan data Berdasarkan Kota
                      df_all.groupby(by=['City'])['Customer ID']\
                      #diagregatkan dengan menghitung jumlahnya (Count)
                      .count()\
                      #mengurutkan data
                      .sort_values(ascending=False)\
                      #mereset nama header
                      .reset_index(name = 'Distribution by City')
                      )
data1

data2 = pd.DataFrame(\
                      #Megelompokkan data Berdasarkan Kota
                      df_all[df_all["Unpaid Tagging"]==1].groupby(by=['City'])['Customer ID']\
                      #diagregatkan dengan menghitung jumlahnya (Count)
                      .count()\
                      #mengurutkan data
                      .sort_values(ascending=False)\
                      #mereset nama header
                      .reset_index(name = 'Distribution by City')
                      )
data2

"""SEBARAN NASABAH BERDASARKAN USIA
* Nasabah Keseluruhan
* Nasabah GAGAL BAYAR
"""

data3 = pd.DataFrame(\
                      #Megelompokkan data Berdasarkan Kota
                      df_all.groupby(by=['Age'])['Customer ID']\
                      #diagregatkan dengan menghitung jumlahnya (Count)
                      .count()\
                      #mereset nama header
                      .reset_index(name = 'Distribution by Age')
                      )
data3.sort_values(by=['Age'], ascending=True, inplace = True )

data3.plot(x= 'Age',y=['Distribution by Age'], kind='bar', title = 'Distribution by Age',
           grid = True, xlabel= 'AGE', ylabel = '# People', rot = 0, table = False, secondary_y=False, figsize=(12,7))

data4 = pd.DataFrame(\
                      #Megelompokkan data Berdasarkan Kota
                      df_all[df_all['Unpaid Tagging']==1].groupby(by=['Age'])['Customer ID']\
                      #diagregatkan dengan menghitung jumlahnya (Count)
                      .count()\
                      #mereset nama header
                      .reset_index(name = 'Distribution by Age')
                      )
data4.sort_values(by=['Age'], ascending=True, inplace = True )

data4.plot(x= 'Age',y=['Distribution by Age'], kind='bar', title = 'Distribution by Age',
           grid = True, xlabel= 'AGE', ylabel = '# People', rot = 0, table = False, secondary_y=False, figsize=(12,7))

"""RATA _RATA SALDO NASABAH"""

df_checkbalance= df_all.copy()
df_checkbalance['Total Balance'] = df_checkbalance['Balance Q1'] + df_checkbalance['Balance Q2'] + df_checkbalance['Balance Q3'] + df_checkbalance['Balance Q4']
df_checkbalance['Avg Balance'] = df_checkbalance['Total Balance']/4

data5 = pd.DataFrame(\
                      df_checkbalance.groupby(by=['Unpaid Tagging'])['Total Balance']\
                     .mean()\
                     .reset_index(name = 'Avg Annual Balance')
                     )
data5

data6 = pd.DataFrame(\
                      df_checkbalance.groupby(by=['Unpaid Tagging'])['Avg Balance']\
                     .mean()\
                     .reset_index(name = 'Avg Quarterly Balance')
                     )
data6

"""RATA - RATA KEPEMILIKAN PRODUK"""

df_checkbalance['Avg Product'] = (df_checkbalance['NumOfProducts Q1'] + df_checkbalance['NumOfProducts Q2'] + df_checkbalance['NumOfProducts Q3'] + df_checkbalance['NumOfProducts Q4'])/4

data7= pd.DataFrame(\
                      df_checkbalance.groupby(by=['Unpaid Tagging'])['Avg Product']\
                     .mean()\
                     .reset_index(name = 'Avg Product Owned')
                     )
data7

"""# DATA PREPARATION

Pengecekan data duplicat
"""

df_all.duplicated().sum()

"""Pengecekan Missing data"""

df_all.isnull().sum()

"""Penambahan Variabel Relevan

~ RATA - RATA SALDO DAN PERUBAHAN SALDO SELAMA PERIODE OBESERVASI ~
MELIHAT SALDO DALAM SCOPE OBSERVASI
1. EKSPERIMEN 1 = RATA RATA SALDO NASABAH DALAM 1 TAHUN TERAKHIR DAN PERUBAHAN SALDO DI Q4 TERHADAP Q1
2. EKSPERIMEN 2= RATA RATA SALDO NASABAH DALAM 6 BULAN DAN PERUBAHAN SALDO DI Q4 TERHADAP Q2
"""

#EKSPERIMEN 1
df1['Mean Balance'] = (df1['Balance Q1'] + df1['Balance Q2'] + df1['Balance Q3'] + df1['Balance Q4'])/4
df1['Delta Balance']= df1['Balance Q4'] - df1['Balance Q1']

df1.head()

#EKSPERIMEN 2
df2['Mean Balance'] = (df2['Balance Q3']+ df2['Balance Q4'])/2
df2['Delta Balance']= df2['Balance Q4'] - df2['Balance Q2']
df2.head()

"""STATUS KEAKTIVAN

MELIHAT PERIODE NASABAH AKTIF DALAM SCOPE OBSERVASI
1. EKSPERIMEN 1: KEAKTIFAN NASABAH (DALAM BULAN) 1 TAHUN TERAKHIR
2. EKSPERIMEN 2: KEAKTIFAN NASABAH (DALAM BULAN) 6 BULAN TERAKHIR
"""

# EKSPERIMEN 1

df1['Active Months'] = (df1['ActiveMember Q1'] + df1['ActiveMember Q2'] + df1['ActiveMember Q3'] + df1['ActiveMember Q4'])*3
df1.head()

# EKSPERIMEN2

df2['Active Months'] = (df2['ActiveMember Q3'] + df2['ActiveMember Q4'])*3
df2.head()

"""PENAMBAHAN ATAU PENGURANGAN PRODUCT HOLDING
melihat fluktuasi kepemilikan produk nasabah dalam periode observasi
"""

# eksperimen 1
df1['Diff PH'] = df1['NumOfProducts Q4'] - df1['NumOfProducts Q1']
df1.head()

# Eksperimen 2
df2['Diff PH'] = df2['NumOfProducts Q4'] - df2['NumOfProducts Q2']
df2.head()

"""LAMA KEPEMILIKAN CC DALAM PRIODE OBSERVASI"""

def assign_cr1(df):
  if df['HasCrCard Q1'] == 1:
    return 12
  elif df['HasCrCard Q2'] == 1:
    return 9
  elif df['HasCrCard Q3'] == 1:
    return 6
  else:
    return 3
  return np.nan

#EKSPERIMEN 1
df1['Vintage CR']= df1.apply(assign_cr1, axis=1)
df1.head()

#EKSPERIMEN 2
df2['Vintage CR']= df2.apply(assign_cr1, axis=1)
df2.head()

"""PENGHAPUSAN VARIABEL"""

# STATUS KEPEMILIKAN SUDAH DIGANTIKAN DENGAN LAMA KEPEMILIKAN KARTU KREDIT
df1 = df1.drop(columns=['HasCrCard Q1','HasCrCard Q2','HasCrCard Q3','HasCrCard Q4'])
df2 = df2.drop(columns=['HasCrCard Q1','HasCrCard Q2','HasCrCard Q3','HasCrCard Q4'])

# BALANCE PER QUARTER SUDAH DIGANTIKAN DENGAN RATA RATA SALDO DALAM PERIODE OBSERVASI DAN SELISIH SALDO AWAL DAN AKHIR PERIODE OBSERVASI
df1 = df1.drop(columns =['Balance Q1','Balance Q2','Balance Q3','Balance Q4'])
df2 = df2.drop(columns =['Balance Q1','Balance Q2','Balance Q3','Balance Q4'])

# JUMLAH KEPEMILIKAN PRODUK TELAH DIGANTI DENGAN FLUKTUASI KEPEMILIKAN PRODUK
df1 = df1.drop(columns=['NumOfProducts Q1','NumOfProducts Q2','NumOfProducts Q3','NumOfProducts Q4'])
df2 = df2.drop(columns=['NumOfProducts Q1','NumOfProducts Q2','NumOfProducts Q3','NumOfProducts Q4'])

#STATUS KEAKTIFAN NASABAH TELAH  DIGANTI DENGAN STATUS KEAKTIFAN DALAM BULAN
df1 = df1.drop(columns=['ActiveMember Q1','ActiveMember Q2','ActiveMember Q3','ActiveMember Q4'])
df2 = df2.drop(columns=['ActiveMember Q1','ActiveMember Q2','ActiveMember Q3','ActiveMember Q4'])

"""# DATA TRANSFORMATION"""

# PEMISAHAN VARIABEL PREDIKTOR
predictor1 =df1.drop(columns=['Customer ID','Unpaid Tagging'])
predictor2 =df2.drop(columns=['Customer ID','Unpaid Tagging'])

predictor1.head()

predictor2.head()

"""MELAKUKAN ENCODING UNTUK DATA CATEGORY
Variabel Category:
1. Branch code
2. City

untuk branch code perlu diubah menjadi string agar dianggap sebagai data kategori

"""

predictor1['Branch Code'] = predictor1['Branch Code'].astype(str)
predictor2['Branch Code'] = predictor2['Branch Code'].astype(str)

predictor1.info()

predictor1= pd.get_dummies(predictor1).astype(int)
predictor2= pd.get_dummies(predictor2).astype(int)

predictor1.head()

predname = predictor1.columns
predname_num= predictor1.columns[0:7]
predname_cat = predictor1.columns[7:31]

predname

X1_num = predictor1[predname_num]
X1_cat = predictor1[predname_cat]
X2_num = predictor2[predname_num]
X2_cat = predictor2[predname_cat]

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X1_num = pd.DataFrame(scaler.fit_transform(X1_num))
X1_num.head()

X1_num.columns= predname_num
X1_num.head()

scaler = StandardScaler()
X2_num = pd.DataFrame(scaler.fit_transform(X2_num))
X2_num.head()

X2_num.columns= predname_num
X2_num.head()

"""MENGGABUNGKAN DATA PREDIKTOR"""

X1 = pd.concat([X1_num,X1_cat],axis=1)
X2 = pd.concat([X2_num,X2_cat],axis=1)

y1 = df1['Unpaid Tagging']
y2 = df2['Unpaid Tagging']

"""MEMPERSIAPKAN DATA UNTUK VALIDATION"""

df1_val = df_val.copy()
df2_val = df_val.copy()

"""Penambahan Variabel Relevan"""

#RATA RATA BALANCE DAN PERUBAHAN SALDO
df1_val['Mean Balance'] = (df1_val['Balance Q2'] + df1_val['Balance Q3'] + df1_val['Balance Q4'] + df1_val['Balance Q5'])/4
df1_val['Delta Balance']= df1_val['Balance Q5'] - df1_val['Balance Q2']

df2_val['Mean Balance'] = (df2_val['Balance Q4'] + df2_val['Balance Q5'])/2
df2_val['Delta Balance']= df2_val['Balance Q5'] - df2_val['Balance Q4']

#Active Month
df1_val['Active Months'] = (df1_val['ActiveMember Q2'] + df1_val['ActiveMember Q3'] + df1_val['ActiveMember Q4'] + df1_val['ActiveMember Q5'])*3
df2_val['Active Months'] = (df2_val['ActiveMember Q4'] + df2_val['ActiveMember Q5'])*3

"""PENAMBAHAN PENGURANGAN PRODUK HOLDING"""

df1_val['Diff PH'] = df1_val['NumOfProducts Q5'] - df1_val['NumOfProducts Q2']
df2_val['Diff PH'] = df2_val['NumOfProducts Q5'] - df2_val['NumOfProducts Q4']

"""LAMA KEPEMILIKAN KARTU KREDIT"""

def assign_cr2(df):
  if df['HasCrCard Q2'] == 1:
    return 12
  elif df['HasCrCard Q3'] == 1:
    return 9
  elif df['HasCrCard Q4'] == 1:
    return 6
  else:
    return 3
  return np.nan

df1_val['Vintage CR']= df1_val.apply(assign_cr2, axis=1)
df2_val['Vintage CR']= df2_val.apply(assign_cr2, axis=1)

"""Penghapusan Variabel"""

df1_val = df1_val.drop(columns=['HasCrCard Q2','HasCrCard Q3','HasCrCard Q4','HasCrCard Q5'])
df2_val = df2_val.drop(columns=['HasCrCard Q2','HasCrCard Q3','HasCrCard Q4','HasCrCard Q5'])

df1_val = df1_val.drop(columns =['Balance Q2','Balance Q3','Balance Q4','Balance Q5'])
df2_val = df2_val.drop(columns =['Balance Q2','Balance Q3','Balance Q4','Balance Q5'])

df1_val = df1_val.drop(columns=['NumOfProducts Q2','NumOfProducts Q3','NumOfProducts Q4','NumOfProducts Q5'])
df2_val = df2_val.drop(columns=['NumOfProducts Q2','NumOfProducts Q3','NumOfProducts Q4','NumOfProducts Q5'])

df1_val = df1_val.drop(columns=['ActiveMember Q2','ActiveMember Q3','ActiveMember Q4','ActiveMember Q5'])
df2_val = df2_val.drop(columns=['ActiveMember Q2','ActiveMember Q3','ActiveMember Q4','ActiveMember Q5'])

df1_val.head()

"""PEMILIHAN VARIABEL PREDIKTOR"""

prediktor1_val= df1_val[df1_val.columns.difference(['Customer ID','Unpaid Tagging'])]
prediktor2_val= df2_val[df2_val.columns.difference(['Customer ID','Unpaid Tagging'])]

prediktor1_val['Branch Code'] = prediktor1_val['Branch Code'].astype(str)
prediktor2_val['Branch Code'] = prediktor2_val['Branch Code'].astype(str)

predictor1_val= pd.get_dummies(prediktor1_val).astype(int)
prediktor2_val= pd.get_dummies(prediktor2_val).astype(int)

X1_num_val = predictor1_val[predname_num]
X1_cat_val = predictor1_val[predname_cat]
X2_num_val = prediktor2_val[predname_num]
X2_cat_val = prediktor2_val[predname_cat]

X1_num_val = pd.DataFrame(scaler.fit_transform(X1_num_val))
X1_num_val.columns = predname_num
X2_num_val = pd.DataFrame(scaler.fit_transform(X2_num_val))
X2_num_val.columns = predname_num

X1_val= pd.concat([X1_num_val,X1_cat_val],axis=1)
X2_val= pd.concat([X2_num_val,X2_cat_val],axis=1)

y1_val = df1_val['Unpaid Tagging']
y2_val = df2_val['Unpaid Tagging']

"""PENGECEKAN KORELASI
Variabel yang berkolerasi lebih dari 0,7 di drop
"""

corrtest1 = X1.corr().abs()
corrtest2 = X2.corr().abs()

corrtest1.style.background_gradient(cmap='YlGnBu',)

# Membuang nilai redudan pada matriks
upper = corrtest1.where(np.triu(np.ones(corrtest1.shape), k=1).astype(bool))

# Mencari nilai yang Berkolerasi diatas 0,7
to_drop= [column for column in upper.columns if any(upper[column]>0.7)]

# Menghapus kolom yang berkorelasi diatas 0.7
X1 = X1.drop(X1[to_drop], axis=1)
X1_val = X1_val.drop(X1_val[to_drop], axis=1)

X1.head()

# Membuang nilai redudan pada matriks
upper2 = corrtest2.where(np.triu(np.ones(corrtest2.shape), k=1).astype(bool))

# Mencari nilai yang Berkolerasi diatas 0,7
to_drop2= [column for column in upper.columns if any(upper2[column]>0.7)]

# Menghapus kolom yang berkorelasi diatas 0.7
X2 = X2.drop(X2[to_drop], axis=1)
X2_val = X2_val.drop(X2_val[to_drop], axis=1)

"""# SPLITTING TRAIN TEST DATA

"""

from sklearn.model_selection import train_test_split

X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.3, random_state=30, stratify=y1)
X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.3, random_state=30, stratify=y2)

"""# MODELLING
Menggunakan 3 Algoritma:
1. Logistic Regressison
2. Gradient Boosting
3. Random Forest

## LOGISTIC REGRESSION
MELAKUKAN DEFINISI HYPERPARAMETER
"""

penalty=['l2']
tol=[0.001,0.0001,0.00001]
C=[100.0,10.0,1.00,0.1,0.01,0.001]
fit_intercept= [True,False]
intercept_scaling=[1.0,0.75,0.5,0.25]
solver=['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']
max_iter=[14000]

param_distribution = dict(penalty=penalty,tol=tol,C=C,fit_intercept=fit_intercept,intercept_scaling=intercept_scaling,solver=solver,max_iter=max_iter)

#MELAKUKAN PENCARIAN HYPERPARAMETER TERBAIK
from sklearn.model_selection import GridSearchCV

"""Eksperimen 1"""

import time

#CV = CROSS VALIDATION
logreg = LogisticRegression()
grid= GridSearchCV(estimator=logreg, param_grid=param_distribution,scoring='recall', cv=5, n_jobs=1)

start_time= time.time()
grid_result= grid.fit(X1_train,y1_train)

# Summarize Result
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
print("Execution time: " + str((time.time() - start_time)) + ' ms')

"""Eksperimen 2"""

grid= GridSearchCV(estimator=logreg, param_grid=param_distribution,scoring='recall', cv=5, n_jobs=1)

start_time= time.time()
grid_result= grid.fit(X2_train,y2_train)

# Summarize Result
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
print("Execution time: " + str((time.time() - start_time)) + ' ms')

grid.score(X2_test,y2_test)

"""## GRADIENT BOOSTING"""

gbparam = {'max_depth':[5,10,15],'gamma':[0.0,0.1,0.2,0.3], 'n_estimators':[25,50,75,100], 'learning_rate':[0.05, 0.1, 0.2, 0.3], 'scale_pos_weight':[1,3]}
score ={'accuracy':make_scorer(accuracy_score), 'precision':make_scorer(precision_score), 'recall':make_scorer(recall_score), 'f1':make_scorer(f1_score)}

"""* GAMMA = nilai minimal loss reduction yang dibutuhkan saat pemecahan cabang. semakin besar nilai gamma yang ditetapkan, model yang dibangun akan lebih konservatif dan memungkinkan terjadinya under fitting
* Learning_rate = tingkat penyesuaian bobot fitur. Dalam pembangunan model, setiap iterasi menghasilkan bobot untuk fitur fitur yang dimiliki. Learning_rate berguna untuk membantu menyusutkan nilai bobot tersebut agar model yang dibangun tidak mengalmi overfiting.
* Scale_pos_weight= pengaturan bobot antara kelas positif dan negatif. hyperparameter ini sangat berguna jika dataset yang digunakan merupakan imbalance data set. nilai yang biasa digunakan yaitu jumlah dari majority class dibagi dengan jumlah minority class

EKSPERIMEN 1
"""

GB_grid=GridSearchCV(XGBClassifier(), gbparam, cv=5, refit='recall', verbose=0, n_jobs=-1, scoring= score)

start_time= time.time()
GB_grid.fit(X1_train,y1_train)

#summarize result
print("Best: %f using %s" % (GB_grid.best_score_, GB_grid.best_params_))
print("Execution time: " + str((time.time() - start_time)) + ' ms')

"""EKSPERIMEN 2"""

GB_grid2=GridSearchCV(XGBClassifier(), gbparam, cv=5, refit='recall', verbose=0, n_jobs=-1, scoring= score)

start_time= time.time()
GB_grid.fit(X2_train,y2_train)

#summarize result
print("Best: %f using %s" % (GB_grid.best_score_, GB_grid.best_params_))
print("Execution time: " + str((time.time() - start_time)) + ' ms')

"""## RANDOM FOREST"""

param = {'max_depth':[5,10,15,20], 'max_features':['auto','sqrt','log2'], 'n_estimators':[25,50,75,100,125], 'min_samples_split':[2,3,5,7]}
score= {'accuracy':make_scorer(accuracy_score), 'precision':make_scorer(precision_score), 'recall':make_scorer(recall_score), 'f1':make_scorer(f1_score)}

"""* Max_depth = jumlah maksimal pemecahan cabang atau level dalam satu pohon. Semakin besar nilai max_depth yang ditetapkan, model akan semakin presisi dalam menggolongkan data suatu kelas. Akan tetapi nilai Max_depth yang semakin besar juga dapat menyebabkan overfitting
* Max_feature = jumlah maksimal max_features yang dipertimbangkan ketika melakukan pemecahan cabang(splitting mode). Sama halnya dengan max_depth, semakin banyak jumlah fitur yang dipertimbangkan dalam pemecahan cabang, akan semakin detail hasil yang didapatkan akan tetapi dapat membuat model overfitting dengan data trainingnya
* N_estimator = jumlah pohon yang dibangun. semakin banyak pohon, tingkat akurasi yang didapatkan menjadi lebih baik mengingat random forest menggunakan konsep majority vote dalam melakukan klasifikasi. Akan tetapi semakin banyak pohon yang dibangun, waktu komputasi yang dibutuhkan juga semakin tinggi
* Min_sample_split = jumlah sample data minimal pada sebuah internal node. Nilai besar yang dapat membuat model dibangun lebih konservatif. Akan tetapi, jika teralalu besar dapat menyebabkan model yang dibangun overfittingm

Eksperimen 1
"""

RF_Grid = GridSearchCV(RandomForestClassifier(), param, cv=5, refit='recall', verbose=0, n_jobs=-1, scoring= score)

start_time= time.time()
RF_Grid.fit(X1_train,y1_train)

#summarize result
print("Best: %f using %s" % (RF_Grid.best_score_, RF_Grid.best_params_))
print("Execution time: " + str((time.time() - start_time)) + ' ms')

"""Eksperimen 2"""

RF_Grid2 = GridSearchCV(RandomForestClassifier(), param, cv=5, refit='recall', verbose=0, n_jobs=-1, scoring= score)

start_time= time.time()
RF_Grid2.fit(X2_train,y2_train)

#summarize result
print("Best: %f using %s" % (RF_Grid2.best_score_, RF_Grid2.best_params_))
print("Execution time: " + str((time.time() - start_time)) + ' ms')

"""# EVALUATION

## LOGISTIK REGRESSION

Eksperimen 1
"""

y1_pred = grid.predict(X1_test)

print("Accuracy: ", metrics.accuracy_score(y1_test,y1_pred))
print("Recall: ", metrics.recall_score(y1_test,y1_pred))
metrics.completeness_score

y1_pred_val= grid.predict(X1_val)

print("Accuracy: ", metrics.accuracy_score(y1_val,y1_pred_val))
print("Recall: ", metrics.recall_score(y1_val,y1_pred_val))
metrics.completeness_score

df_imp1= mean_score_decrease(X1_train, y1_train, grid, plot=True, topk=20)

"""Eksperimen 2"""

y2_pred = grid.predict(X2_test)

print("Accuracy: ", metrics.accuracy_score(y2_test,y2_pred))
print("Recall: ", metrics.recall_score(y2_test,y2_pred))
metrics.completeness_score

y2_pred_val= grid.predict(X2_val)

print("Accuracy: ", metrics.accuracy_score(y2_val,y2_pred_val))
print("Recall: ", metrics.recall_score(y2_val,y2_pred_val))
metrics.completeness_score

df_imp2= mean_score_decrease(X2_train, y2_train, grid, plot=True, topk=20)

"""## GRADIENT BOOSTING

Eksperimen 1
"""

y11_pred = GB_grid.predict(X1_test)

print("Accuracy: ", metrics.accuracy_score(y1_test,y11_pred))
print("Recall: ", metrics.recall_score(y1_test,y11_pred))
metrics.completeness_score

y12_pred= GB_grid.predict(X1_val)

print("Accuracy: ", metrics.accuracy_score(y1_val,y12_pred))
print("Recall: ", metrics.recall_score(y1_val,y12_pred))
metrics.completeness_score

df_imp3= mean_score_decrease(X1_train,y1_train, GB_grid, plot=True, topk=20)

"""Eksperimen 2"""

y12_pred = GB_grid.predict(X2_test)

print("Accuracy: ", metrics.accuracy_score(y2_test,y12_pred))
print("Recall: ", metrics.recall_score(y2_test,y12_pred))
metrics.completeness_score

y22_pred = GB_grid.predict(X2_val)

print("Accuracy: ", metrics.accuracy_score(y2_val,y22_pred))
print("Recall: ", metrics.recall_score(y2_val,y22_pred))
metrics.completeness_score

df_imp4 = mean_score_decrease(X2_train,y2_train, GB_grid, plot=True, topk=20)

"""## RANDOM FOREST

EKSPERIMEN 1
"""

y12_pred= Rf_Grid.predict(X1_test)

print("Accuracy: ", metrics.accuracy_score(y1_test,y12_pred))
print("Recall: ", metrics.recall_score(y1_test,y12_pred))
metrics.completeness_score

y12_pred_val= Rf_Grid.predict(X1_val)

print("Accuracy: ", metrics.accuracy_score(y1_val,y12_pred_val))
print("Recall: ", metrics.recall_score(y1_val,y12_pred_val))
metrics.completeness

df_imp5 = mean_score_decrease(X1_train,y1_train, RF_Grid, plod=True, topk=20)

"""EKSPERIMEN 2"""

y22_pred = RF_Grid2.predict(X2_test)

print("Accuracy: ", metrics.accuracy_score(y2_test,y22_pred))
print("Recall: ", metrics.recall_score(y2_test,y22_pred))
metrics.completeness_score

y22_pred_val = RF_Grid2.predict(X2_val)

print("Accuracy: ", metrics.accuracy_score(y2_val,y22_pred_val))
print("Recall: ", metrics.recall_score(y2_val,y22_pred_val))
metrics.completeness_score

df_imp6= mean_score_decrease(X2_train,y2_train,RF_Grid2, plot=True, topk=20)

"""# KESIMPULAN

Dalam semua model rata - rata memiliki akurasi diatas 60% namun memiliki recall di bawah 40%. Artinya, Masih banyak nasabah yang sebenarnya berpotensi gagal bayar namun diprediksi tidak akan gagal bayar. Sehingga bisa disimpulkan bahwa dalam iterasi pembangunan model kali ini, objektif yang diinginkan masih belum tercapai

Solusi pengembangan yang bisa dilakukan kedepannya:
1. Memperbanyak sample (jumlah nasabah dengan asumsi data set yang tersedia saat ini bukan populasi nasabah)
2. Melakukan Oversampling terhadap kelas minoritas (gagal bayar) agar pembangunan model tidak bias
3. Memperluas horizon waktu
4. Mencoba variasi variabel lainnya (menambah variabel baru, atau membuang variabel yang memiliki nilai importance rendah pada hasil akhir)
5. Mencoba memperluas kombinasi hyperparameter dalam pembangunan model
6. Mencoba algoritma Supervised Machine Learning lainnya
"""